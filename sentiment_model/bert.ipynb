{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a0a4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW # Import AdamW from torch.optim\n",
    "from transformers import get_scheduler\n",
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36afb4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('E:\\sentiment_analysis\\notebooks\\annotated_data.csv')\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886fe1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: build insightful EDA best for sentiment analysis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Distribution of Sentiment Labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='sentiment_label', data=df)\n",
    "plt.title('Distribution of Sentiment Labels')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Analyze Text Length\n",
    "df['text_length'] = df['Review'].apply(len)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df['text_length'], kde=True)\n",
    "plt.title('Distribution of Text Length')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Relationship between Text Length and Sentiment\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='sentiment_label', y='text_length', data=df)\n",
    "plt.title('Text Length vs. Sentiment')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Text Length')\n",
    "plt.show()\n",
    "\n",
    "# Word Cloud Visualization (requires wordcloud library)\n",
    "#!pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Combine all text for each sentiment\n",
    "sentiment_texts = {}\n",
    "for sentiment in df['sentiment_label'].unique():\n",
    "  sentiment_texts[sentiment] = ' '.join(df[df['sentiment_label'] == sentiment]['processed_text'])\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, sentiment in enumerate(sentiment_texts):\n",
    "  plt.subplot(2, 2, i+1)\n",
    "  wordcloud = WordCloud(width=800, height=400, background_color='white').generate(sentiment_texts[sentiment])\n",
    "  plt.imshow(wordcloud, interpolation='bilinear')\n",
    "  plt.title(f'Word Cloud for {sentiment} Sentiment')\n",
    "  plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Explore most frequent words for each sentiment\n",
    "from collections import Counter\n",
    "\n",
    "def plot_most_frequent_words(sentiment, n=10):\n",
    "    text = ' '.join(df[df['sentiment_label'] == sentiment]['processed_text']).lower()\n",
    "    words = text.split()\n",
    "    word_counts = Counter(words)\n",
    "    most_common_words = word_counts.most_common(n)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(*zip(*most_common_words))\n",
    "    plt.title(f'Most frequent words for {sentiment} sentiment')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.show()\n",
    "\n",
    "for sentiment in df['sentiment_label'].unique():\n",
    "    plot_most_frequent_words(sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b32e0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Display original class distribution\n",
    "print(\"Original class distribution:\", Counter(df['sentiment_label']))\n",
    "\n",
    "# Convert text data into numerical format (for SMOTE)\n",
    "df['cleaned_review'] = df['cleaned_review'].astype(str)  # Ensure all reviews are string type\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['cleaned_review']) # Fit and transform text data into TF-IDF matrix\n",
    "y = df['sentiment_label']\n",
    "\n",
    "# Apply SMOTE to balance classes\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Create a balanced DataFrame\n",
    "# Apply SMOTE to balance classes\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Create a balanced DataFrame\n",
    "reviews = []\n",
    "for sparse_matrix in vectorizer.inverse_transform(X_resampled):\n",
    "  reviews.append(' '.join(sparse_matrix))\n",
    "\n",
    "df_balanced = pd.DataFrame({'Review': reviews, 'Sentiment': y_resampled}) # Convert back to text for dataframe.\n",
    "\n",
    "\n",
    "# Display new class distribution\n",
    "print(\"Balanced class distribution:\", Counter(df_balanced['Sentiment']))\n",
    "\n",
    "# Save the new balanced dataset\n",
    "df_balanced.to_csv(\"balanced_reviews.csv\", index=False)\n",
    "print(\"Balanced dataset saved as 'balanced_reviews.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24405a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your dataset (Ensure it has 'text' and 'label' columns)\n",
    "df = pd.read_csv('balanced_reviews.csv')\n",
    "\n",
    "# Ensure label encoding is correct\n",
    "label_mapping = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "df['Sentiment'] = df['Sentiment'].map(label_mapping).astype(int)  # Encode labels\n",
    "\n",
    "# Splitting into train, validation, and test sets (80-10-10 split)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['Review'], df['Sentiment'], test_size=0.2, random_state=42, stratify=df['Sentiment']\n",
    ")\n",
    "\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    test_texts, test_labels, test_size=0.5, random_state=42, stratify=test_labels\n",
    ")\n",
    "\n",
    "# Convert to DataFrame and Save\n",
    "train_df = pd.DataFrame({'Review': train_texts, 'Sentiment': train_labels})\n",
    "val_df = pd.DataFrame({'Review': val_texts, 'Sentiment': val_labels})\n",
    "test_df = pd.DataFrame({'Review': test_texts, 'Sentiment': test_labels})\n",
    "\n",
    "train_df.to_csv('train.csv', index=False)\n",
    "val_df.to_csv('val.csv', index=False)\n",
    "test_df.to_csv('test.csv', index=False)\n",
    "\n",
    "print(\"Data Splitting Completed! Train:\", len(train_df), \"Val:\", len(val_df), \"Test:\", len(test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c9728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load Pretrained BERT Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenizing function\n",
    "def tokenize_data(texts, labels):\n",
    "    # Convert any non-string values to strings\n",
    "    texts = texts.astype(str)\n",
    "    encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=512)\n",
    "    return encodings, labels\n",
    "\n",
    "# Tokenizing Train, Validation, and Test sets\n",
    "train_encodings, train_labels = tokenize_data(train_df['Review'], train_df['Sentiment'])\n",
    "val_encodings, val_labels = tokenize_data(val_df['Review'], val_df['Sentiment'])\n",
    "test_encodings, test_labels = tokenize_data(test_df['Review'], test_df['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942c3a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels.to_numpy(), dtype=torch.long)  # Convert Series to NumPy array first\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "\n",
    "# Convert encodings to PyTorch datasets\n",
    "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "val_dataset = SentimentDataset(val_encodings, val_labels)\n",
    "test_dataset = SentimentDataset(test_encodings, test_labels)\n",
    "\n",
    "print(\"Datasets Prepared!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6bf83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Define number of classes (adjust based on your dataset)\n",
    "num_labels = len(set(train_labels))\n",
    "\n",
    "# Load pretrained BERT model for classification\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "\n",
    "print(\"Model Loaded Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1905fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 8 #change from 16\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"DataLoaders Ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0ff21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Store training history\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0, 0, 0\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "\n",
    "    for batch in loop:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        # batch[\"labels\"] = batch.pop(\"Sentiment\")  # ✅ Rename \"Sentiment\" to \"labels\"\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = loss_fn(outputs.logits, batch[\"labels\"])\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "        total += batch[\"labels\"].size(0)\n",
    "\n",
    "        batch_accuracy = (predictions == batch[\"labels\"]).float().mean().item()\n",
    "\n",
    "        loop.set_description(f\"Training\")\n",
    "        loop.set_postfix(loss=loss.item(), accuracy=batch_accuracy)\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = correct / total\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            loss = loss_fn(outputs.logits, batch[\"labels\"])\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "            total += batch[\"labels\"].size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = correct / total\n",
    "\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f} | Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# **Plot Training and Validation Loss**\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(range(1, epochs+1), train_losses, label=\"Train Loss\", marker=\"o\")\n",
    "plt.plot(range(1, epochs+1), val_losses, label=\"Validation Loss\", marker=\"o\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# **Plot Training and Validation Accuracy**\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(range(1, epochs+1), train_accuracies, label=\"Train Accuracy\", marker=\"o\")\n",
    "plt.plot(range(1, epochs+1), val_accuracies, label=\"Validation Accuracy\", marker=\"o\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training & Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTraining Complete!\")\n",
    "# Save the model\n",
    "model.save_pretrained(\"sentiment_model\")\n",
    "tokenizer.save_pretrained(\"sentiment_model\")\n",
    "\n",
    "print(\"Model and tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfe30b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load test data from CSV\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Replace NaN values in the 'Review' column with an empty string\n",
    "test_df['Review'] = test_df['Review'].fillna('')  # Fill NaN with empty string\n",
    "\n",
    "# Assume the CSV has columns 'text' and 'labels' for text and labels respectively.\n",
    "Review = test_df['Review'].tolist()\n",
    "# ✅ Changed 'sentiment_label' to 'Sentiment' to match the column name in test_df\n",
    "sentiment_label = test_df['Sentiment'].tolist()\n",
    "\n",
    "# Initialize tokenizer (assuming you're using a BERT-based model)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Create Dataset class to handle tokenization and batching\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_dataset = TestDataset(Review, sentiment_label, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a95978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Evaluation using Test Loader\n",
    "model.eval()  # Set model to evaluation mode\n",
    "test_loss, correct, total = 0, 0, 0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():  # No gradients are needed for evaluation\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = loss_fn(outputs.logits, batch[\"labels\"])\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "        total += batch[\"labels\"].size(0)\n",
    "\n",
    "        all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "        all_preds.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Calculate average test loss and accuracy\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "test_accuracy = correct / total\n",
    "\n",
    "# Calculate other metrics: Precision, Recall, F1-Score\n",
    "test_precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "test_recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "test_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Get unique labels from your data\n",
    "unique_labels = np.unique(all_labels)\n",
    "num_classes = len(unique_labels) # get the actual number of classes\n",
    "\n",
    "# Create class labels dynamically\n",
    "class_labels = [f\"Class {i}\" for i in range(num_classes)]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_labels,  # Use dynamic labels\n",
    "            yticklabels=class_labels)  # Use dynamic labels\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
